{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> DATA PREP and PRE-REQUISITES </h2>\n",
    "\n",
    "- Download the script image_capture.py to your local machine. This script will use your webcam on your laptop to capture images from your webcam and save it to your disk\n",
    "- The script could function a little differently on a MAC vs Windows. I have tested it on MAC and it works very well\n",
    "- The images you capture depends on your use case. I took a simple video that detects if a person is wearing a mask. I took 3 picture types (labels) for it\n",
    "-    mask\n",
    "-    nomask\n",
    "-    incmask\n",
    "\n",
    "- You can easily change that to hardhat or any other safety gears. Make sure you are clear on what you want to capture as your labels.\n",
    "- if you take more than 3 labels then you need to modify the script accordingly.\n",
    "- This script will save images in the relevant folders based on your labels.\n",
    "- Once you capture images using this script, create the following folder structure.\n",
    "    train_data\n",
    "       images\n",
    "         train\n",
    "         val\n",
    "       labels\n",
    "         train\n",
    "         val\n",
    "- Divide your pictures in such a way that 80% of your pictures are used for training and 20% are used for validation and then load these images into train and val folders respectively\n",
    "- For the labels, we will visit <a href = \"https://www.makesense.ai/\">Makesense.ai</a> where we will label our training and validation images, the instructor will help you in this step. You can also use Amazon Sagemaker GroundTruth to label these images. However, the augmented manifest file that is generated once the GroundTruth labeling job is complete needs to be converted into a format that is compatible with the yolov5 model. makesense.ai provides us yolo format out of the box. From a security standpoint, these images are NOT uploaded to makesense.ai when you are labeling them\n",
    "- Once the labels are created, download them and add them to the train and val labels folder.\n",
    "- As a final step, zip the entire train_data folder and bring the zip file into the Jupyter Notebook\n",
    "\n",
    "\n",
    "Once the zip folder is uploaded to Studio, unzip it using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip train_data.zip -d train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> You can delete the zip folder once extracted </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Clone the ultralytics reposity for yolov5. We will use the yolov5s pre-trained model. The \"s\" stands for small </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Change the path to yolov5 once the repo is cloned</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Install the required dependencies<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> set an env variable <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Configuration Steps</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the coco128.yaml file located at yolov5/data\n",
    "- Modify this download file to include your labels and data locations. The coco128.yaml file contains over 90 classes the yolov5 model is trained on. You can remove all those classes and add your own. This is what I have done.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "```\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "\n",
    "train: ../train_data/images/train  # train images (relative to 'path') 128 images\n",
    "val: ../train_data/images/val  # val images (relative to 'path') 128 images\n",
    "test:  # test images (optional)\n",
    "\n",
    "# Classes\n",
    "nc: 3  # number of classes\n",
    "names: ['mask','nomask','incmask']  # class names\n",
    "\n",
    "</code>\n",
    "```\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Save the file and give it a custom name and upload it to the yolov5/data folder. I have named my file as coco_dataset.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TRAINING </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We will use transfer learning and take advantage of the awesome yolov5 pre trained model to perform our customized dataset training. yolov5 requires annotated images(labels) with the same names as the image names. We did that in our pre-requisite steps. The labels are nothing but the class and the bounding box locations that we drew on the makesense.api site. <p>\n",
    "    \n",
    "    \n",
    "<p>There are many hyper-parameters that can be tweaked for this model. The file is located at yolov5/data/hyps. This file has many hyper parameters such as learning rate, weight, decays etc etc. However, we will train the model with the default hyperparameters before we start tuning them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 416 --batch 1 --epochs 100 --data coco_dataset.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> INFERENCING </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Use the weights generated by the training job after \"n\" epochs. This weight indicates the best trained model for your use case. You can add multiple sources for detection from images to videos to RTSP streams or even youtube videos for inferencing </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights runs/train/exp2/weights/best.pt --img 640 --conf 0.25 --source ../outpy.avi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> EXPORT </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Once the model is trained it can be exported to a onnx format. It can be compiled by Amazon Sagemaker neo to deploy to the edge. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Exporting to ONNX format </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
